{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STvPl0140Tf2"
      },
      "outputs": [],
      "source": [
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "# Fetch the YouTube transcript\n",
        "def fetch_transcript(video_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetches the transcript of a YouTube video using its URL.\n",
        "\n",
        "    Args:\n",
        "        video_url (str): The YouTube video URL.\n",
        "\n",
        "    Returns:\n",
        "        str: Concatenated transcript text or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract video ID\n",
        "        if \"youtube.com\" in video_url:\n",
        "            video_id = video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
        "        elif \"youtu.be\" in video_url:\n",
        "            video_id = video_url.split(\"/\")[-1].split(\"?\")[0]\n",
        "        else:\n",
        "            return \"Error: Invalid YouTube URL.\"\n",
        "\n",
        "        # Fetch transcript\n",
        "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        full_transcript = \" \".join([item[\"text\"] for item in transcript])\n",
        "        return full_transcript\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching transcript: {e}\"\n",
        "\n",
        "\n",
        "# Chunk transcript into sentences\n",
        "def split_into_chunks(transcript: str) -> list:\n",
        "    \"\"\"\n",
        "    Splits the transcript into smaller chunks for retrieval.\n",
        "\n",
        "    Args:\n",
        "        transcript (str): The full transcript text.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of transcript chunks (sentences).\n",
        "    \"\"\"\n",
        "    return sent_tokenize(transcript)\n",
        "\n",
        "\n",
        "# Retrieve the most relevant chunks using BM25\n",
        "def retrieve_relevant_chunks(chunks: list, question: str, top_n: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Retrieves the most relevant transcript chunks using BM25.\n",
        "\n",
        "    Args:\n",
        "        chunks (list): List of transcript sentences.\n",
        "        question (str): The user's question.\n",
        "        top_n (int): Number of top relevant chunks to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        str: Concatenated relevant chunks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokenized_chunks = [word_tokenize(chunk.lower()) for chunk in chunks]\n",
        "        bm25 = BM25Okapi(tokenized_chunks)\n",
        "        query_tokens = word_tokenize(question.lower())\n",
        "        scores = bm25.get_scores(query_tokens)\n",
        "\n",
        "        # Get top-n chunks based on scores\n",
        "        top_chunks = [chunks[i] for i in sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]]\n",
        "        return \" \".join(top_chunks)\n",
        "    except Exception as e:\n",
        "        return f\"Error retrieving chunks: {e}\"\n",
        "\n",
        "\n",
        "# Generate an answer using T5\n",
        "def answer_question_with_t5(context: str, question: str, model, tokenizer) -> str:\n",
        "    \"\"\"\n",
        "    Generates an answer using the T5 model.\n",
        "\n",
        "    Args:\n",
        "        context (str): The relevant context.\n",
        "        question (str): The question.\n",
        "        model: Pre-trained T5 model.\n",
        "        tokenizer: Tokenizer for the T5 model.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated answer or an error message.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        input_text = f\"question: {question} context: {context}\"\n",
        "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "        # Generate answer\n",
        "        outputs = model.generate(inputs, max_length=100, num_beams=4, early_stopping=True)\n",
        "        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error generating answer: {e}\"\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load T5 model and tokenizer\n",
        "    model_name = \"t5-large\"  # You can use \"t5-base\" for a lighter model\n",
        "    try:\n",
        "        tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "        print(\"Model and tokenizer loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model/tokenizer: {e}\")\n",
        "        exit()\n",
        "\n",
        "    # Example usage\n",
        "    video_url = \"https://youtu.be/6lt5WzBZN4Q\"  # Replace with a real YouTube video URL\n",
        "    question = \"What is this video about\"\n",
        "\n",
        "    # Fetch and process transcript\n",
        "    transcript = fetch_transcript(video_url)\n",
        "    if \"Error\" in transcript:\n",
        "        print(transcript)\n",
        "    else:\n",
        "        # Split transcript into chunks\n",
        "        chunks = split_into_chunks(transcript)\n",
        "\n",
        "        # Retrieve relevant chunks\n",
        "        relevant_context = retrieve_relevant_chunks(chunks, question, top_n=3)\n",
        "        if \"Error\" in relevant_context:\n",
        "            print(relevant_context)\n",
        "        else:\n",
        "            # Generate answer\n",
        "            answer = answer_question_with_t5(relevant_context, question, model, tokenizer)\n",
        "            print(\"\\nQuestion:\", question)\n",
        "            print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cwUw-NC0hUW",
        "outputId": "16319891-ef7c-469b-88bd-eb91be63e4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully.\n",
            "\n",
            "Question: What is this video about\n",
            "Answer: a long time ago an old man lived in a small village in the mountains in the middle of China\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAfj9-_UAzZN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}